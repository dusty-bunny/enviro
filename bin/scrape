#!/bin/bash

#
# Save a web page and all associated components needed to function fully.
# Recurse to pick up multi page stories.
#
PAGES=1
[ $# -eq 0 ] && echo "No web page provided" >&2 && exit 1
SHORTFORM=${1%%http*/}
WEBSITE="https://www.literotica.com/s/${SHORTFORM}"
# WEBSITE=https:"${1#http*:}" ; shift
echo "Website :: <$WEBSITE>"
get_page_count()
{
        local Source=

        Source=$1 ; shift
        [ ! -f "${Source}" ] && return 1
        sed -n -e 's/^.*\([0-9][0-9]*\) *Pages.*/\1/p' ${Source}
}

get_page()
{
	local Website=
	local StoryPage=

	StoryPage="$1"
	Website=${StoryPage%%s/*}
	wget --recursive \
	     --page-requisites \
	     --html-extension \
	     --convert-links \
	     --domains ${Website} \
	     --no-parent \
	     ${StoryPage} >/dev/null 2>&1
	[ -f "${StoryPage#https://}.html" ] || return 1

	echo "${StoryPage#https://}.html retrieved"
	return 0
}

[ $# -ne 0 ] && PAGES=$1 ; shift

#
# Keep pulling the next page until no more pages.
#
get_story()
{
        local Pages=
	local Count=
        local Story=
        local File=

        Story="$1"
	Count=2
	Pages=0
	File="${Story#https://}.html"
	get_page $Story 2>/dev/null
	if Doh=$(grep Pages: $File) 2>/dev/null ; then
		Pages=$(echo $Doh | sed -e 's,^.*>\([0-9][0-9]*\)[	 ]*Pages:.*,\1,')
	fi
	[ $Pages -gt -0 ] && echo "This story is $Pages pages long"
	while [ $Count -le $Pages ] ; do
		get_page "${Story}?page=$Count"
		Count=$(( Count + 1 ))
	done
}


echo "Download the story ${WEBSITE##*/s/}"
get_story "$WEBSITE"

