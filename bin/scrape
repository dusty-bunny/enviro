#!/bin/bash

#
# Save a web page and all associated components needed to function fully.
# Recurse to pick up multi page stories.
#
PAGES=1
[ $# -eq 0 ] && echo "No web page provided" >&2 && exit 1
WEBSITE=https:"${1#http*:}" ; shift

get_page_count()
{
        local Source=

        Source=$1 ; shift
        [ ! -f "${Source}" ] && return 1
        sed -n -e 's/^.*\([0-9][0-9]*\) *Pages.*/\1/p' ${Source}
}

[ $# -ne 0 ] && PAGES=$1 ; shift
while [ $PAGES -ne 0 ] ; do
        NAME="${WEBSITE}?page=$PAGES"
        wget \
             --recursive \
             --page-requisites \
             --html-extension \
             --convert-links \
             --domains ${WEBSITE%%/*} \
             --no-parent \
                ${NAME}
        ls ${NAME#https://}.html
        sed -i -e '/page=/s,https://,file:///home/zglue/,g' -e 's,?page=\([0-9]\)",%3Fpage=\1.html",g' ${NAME#https://}.html
        PAGES=$(( PAGES - 1 ))
done
